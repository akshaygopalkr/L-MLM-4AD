{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20738,
     "status": "ok",
     "timestamp": 1708973011970,
     "user": {
      "displayName": "Akshay Gopalkrishnan",
      "userId": "04646250786258962279"
     },
     "user_tz": 480
    },
    "id": "2GE0dGNfOSL4",
    "outputId": "edd3e205-bd7c-41d3-9c84-826445ec6274",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:24:11.351196Z",
     "start_time": "2024-05-14T22:24:11.347902Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'multi_frame', 'test_eval.json')) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# behavior_test_data = [item for item in test_data if item[0]['Q'] == 'Predict the behavior of the ego vehicle.']\n",
    "\n",
    "# len(behavior_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:24:19.415764Z",
     "start_time": "2024-05-14T22:24:17.226475Z"
    }
   },
   "source": [
    "with open(os.path.join('data', 'QA_dataset_nus', 'v1_1_train_nus_final.json')) as f:\n",
    "    data = json.load(f)\n",
    "with open(os.path.join('data', 'QA_dataset_nus', 'v1_1_val_nus_q_only.json')) as f:\n",
    "    test_data = json.load(f)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:24:20.271457Z",
     "start_time": "2024-05-14T22:24:20.266349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scene_tokens = list(data.keys())\n",
    "random.shuffle(scene_tokens)\n",
    "train_data = scene_tokens[:int(len(data)*0.95)]\n",
    "val_data = scene_tokens[int(len(data)*0.95):]\n",
    "train_data = {scene_token: data[scene_token] for scene_token in train_data}\n",
    "val_data = {scene_token: data[scene_token] for scene_token in val_data}\n",
    "len(train_data), len(val_data), len(test_data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661, 35, 149)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3518,
     "status": "ok",
     "timestamp": 1707617432636,
     "user": {
      "displayName": "Akshay Gopalkrishnan",
      "userId": "04646250786258962279"
     },
     "user_tz": 480
    },
    "id": "HVUG0NQJTNiN",
    "outputId": "8e1f2c59-7bf0-48e1-e29c-b3aa9e49ee29",
    "ExecuteTime": {
     "end_time": "2024-05-08T05:27:14.148744Z",
     "start_time": "2024-05-08T05:27:14.138508Z"
    }
   },
   "source": [
    "\n",
    "# with open(os.path.join('data','multi_frame', 'test_eval.json')) as f:\n",
    "#   data = json.load(f)\n",
    "\n",
    "scene_tokens = list(data.keys())\n",
    "random.shuffle(scene_tokens)\n",
    "train_data = scene_tokens[:int(len(data)*0.85)]\n",
    "val_data = scene_tokens[int(len(data)*0.85):int(len(data)*0.9)]\n",
    "test_data = scene_tokens[int(len(data)*0.9):int(len(data)*0.95)]\n",
    "rag_data = scene_tokens[int(len(data)*0.95):]\n",
    "\n",
    "train_data = {scene_token: data[scene_token] for scene_token in train_data}\n",
    "val_data = {scene_token: data[scene_token] for scene_token in val_data}\n",
    "test_data = {scene_token: data[scene_token] for scene_token in test_data}\n",
    "rag_data = {scene_token: data[scene_token] for scene_token in rag_data}\n",
    "\n",
    "len(train_data), len(val_data), len(test_data), len(rag_data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(591, 35, 35, 35)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28Oq9Zpr1klq"
   },
   "source": [
    "# Number of Frames in DriveLM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1707617436496,
     "user": {
      "displayName": "Akshay Gopalkrishnan",
      "userId": "04646250786258962279"
     },
     "user_tz": 480
    },
    "id": "ClKFpQoBTUDs",
    "outputId": "0b8866e5-96ae-401f-a2b3-1800b2221e18",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:24:24.228532Z",
     "start_time": "2024-05-14T22:24:24.224472Z"
    }
   },
   "source": [
    "num_frames_train = sum([len(train_data[scene_token]['key_frames']) for scene_token in train_data])\n",
    "num_frames_val = sum([len(val_data[scene_token]['key_frames']) for scene_token in val_data])\n",
    "num_frames_test = sum([len(test_data[scene_token]['key_frames']) for scene_token in test_data])\n",
    "total_frames = num_frames_train + num_frames_val + num_frames_test \n",
    "num_frames_train, num_frames_val, num_frames_test, (num_frames_train / total_frames), (num_frames_val / total_frames), (num_frames_test / total_frames)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3853, 219, 799, 0.7910080065694929, 0.04495996715253541, 0.16403202627797167)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx5mIuZg14-H"
   },
   "source": [
    "# Calculating # of QA's in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1707617440170,
     "user": {
      "displayName": "Akshay Gopalkrishnan",
      "userId": "04646250786258962279"
     },
     "user_tz": 480
    },
    "id": "W8lPUBlTTaaM",
    "outputId": "b293192f-7ded-4a01-b960-efc90bb351b7",
    "ExecuteTime": {
     "end_time": "2024-05-14T22:24:27.291301Z",
     "start_time": "2024-05-14T22:24:27.274301Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_qa(d):\n",
    "\n",
    "    num_qa = 0\n",
    "    qa = set()\n",
    "    question_types = defaultdict(int)\n",
    "    \n",
    "    for scene_token in d:\n",
    "    \n",
    "      for frame_token in d[scene_token]['key_frames']:\n",
    "    \n",
    "        qa = qa.union(set(d[scene_token]['key_frames'][frame_token]['QA'].keys()))\n",
    "    \n",
    "        for q in d[scene_token]['key_frames'][frame_token]['QA']:\n",
    "    \n",
    "          question_types[q] += len(d[scene_token]['key_frames'][frame_token]['QA'][q])\n",
    "          num_qa += len(d[scene_token]['key_frames'][frame_token]['QA'][q])\n",
    "\n",
    "    return num_qa, qa, question_types\n",
    "\n",
    "train_qa = count_qa(train_data)\n",
    "val_qa = count_qa(val_data)\n",
    "test_qa = count_qa(test_data)\n",
    "total_qa = (train_qa[0] + val_qa[0] + test_qa[0])\n",
    "\n",
    "train_qa, val_qa, test_qa, train_qa[0] / total_qa, val_qa[0] / total_qa, test_qa[0] / total_qa"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((555576,\n",
       "  {'behavior', 'perception', 'planning', 'prediction'},\n",
       "  defaultdict(int,\n",
       "              {'perception': 306580,\n",
       "               'prediction': 233588,\n",
       "               'planning': 11555,\n",
       "               'behavior': 3853})),\n",
       " (32540,\n",
       "  {'behavior', 'perception', 'planning', 'prediction'},\n",
       "  defaultdict(int,\n",
       "              {'perception': 18380,\n",
       "               'prediction': 13284,\n",
       "               'planning': 657,\n",
       "               'behavior': 219})),\n",
       " (15480,\n",
       "  {'behavior', 'perception', 'planning', 'prediction'},\n",
       "  defaultdict(int,\n",
       "              {'perception': 1543,\n",
       "               'prediction': 6539,\n",
       "               'planning': 6599,\n",
       "               'behavior': 799})),\n",
       " 0.9204434754372129,\n",
       " 0.053910231346794873,\n",
       " 0.025646293215992155)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:24:37.274318Z",
     "start_time": "2024-05-14T22:24:35.029270Z"
    }
   },
   "source": [
    "def save_mf_pairs(data):\n",
    "\n",
    "    multi_frame_qi_pairs = []\n",
    "    num_questions  = 0\n",
    "\n",
    "    for scene_token in list(data.keys()):\n",
    "    \n",
    "      scene_frames = data[scene_token]['key_frames']\n",
    "    \n",
    "      for frame_token in scene_frames:\n",
    "    \n",
    "        frame = scene_frames[frame_token]\n",
    "        frame_data_qa = frame['QA']\n",
    "        frame_data_qa = frame_data_qa['perception'] + frame_data_qa['prediction'] + frame_data_qa['planning'] + frame_data_qa['behavior']\n",
    "    \n",
    "        for i, question in enumerate(frame_data_qa):\n",
    "    \n",
    "          image_paths = scene_frames[frame_token]['image_paths']        \n",
    "          question['id'] = f'{scene_token}_{frame_token}_{i}'\n",
    "          multi_frame_qi_pairs.append((question, image_paths))\n",
    "\n",
    "    for i in range(len(multi_frame_qi_pairs)):\n",
    "\n",
    "      new_cam_set = {cam: '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train' + multi_frame_qi_pairs[i][1][cam][2:] for cam in multi_frame_qi_pairs[i][1]}\n",
    "      multi_frame_qi_pairs[i] = (multi_frame_qi_pairs[i][0], dict(new_cam_set))\n",
    "\n",
    "    return multi_frame_qi_pairs\n",
    "\n",
    "def save_mf_behavior_pairs(data):\n",
    "\n",
    "    multi_frame_qi_pairs = []\n",
    "    num_questions  = 0\n",
    "\n",
    "    for scene_token in list(data.keys()):\n",
    "    \n",
    "      scene_frames = data[scene_token]['key_frames']\n",
    "    \n",
    "      for frame_token in scene_frames:\n",
    "    \n",
    "        frame = scene_frames[frame_token]\n",
    "    \n",
    "        for q in frame['QA']:\n",
    "    \n",
    "          qa_list = frame['QA'][q]\n",
    "          image_paths = scene_frames[frame_token]['image_paths']\n",
    "\n",
    "          if q == 'behavior':\n",
    "    \n",
    "              for question in qa_list:\n",
    "        \n",
    "                multi_frame_qi_pairs.append((question, image_paths))\n",
    "\n",
    "    for i in range(len(multi_frame_qi_pairs)):\n",
    "\n",
    "      new_cam_set = {cam: '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train' + multi_frame_qi_pairs[i][1][cam][2:] for cam in multi_frame_qi_pairs[i][1]}\n",
    "      multi_frame_qi_pairs[i] = (multi_frame_qi_pairs[i][0], dict(new_cam_set))\n",
    "\n",
    "    return multi_frame_qi_pairs\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "multi_frame_train = save_mf_pairs(train_data)\n",
    "multi_frame_val = save_mf_pairs(val_data)\n",
    "multi_frame_test = save_mf_pairs(test_data)\n",
    "random.shuffle(multi_frame_train), random.shuffle(multi_frame_val)\n",
    "len(multi_frame_train), len(multi_frame_val), len(multi_frame_test), multi_frame_test[:10]\n",
    "# multi_frame_test_behavior[:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(555576,\n",
       " 32540,\n",
       " 15480,\n",
       " [({'Q': 'What are the important objects in the current scene? Those objects will be considered for the future reasoning and driving decision.',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [2],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_0'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'What is the moving status of object <c1,CAM_BACK,384.2,477.5>? Please select the correct answer from the following options: A. Turn right. B. Drive backward. C. Going ahead. D. Turn left.',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_1'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'What object should the ego vehicle notice first when the ego vehicle is getting to the next possible location? What is the state of the object that is first noticed by the ego vehicle and what action should the ego vehicle take? What object should the ego vehicle notice second when the ego vehicle is getting to the next possible location? What is the state of the object perceived by the ego vehicle as second and what action should the ego vehicle take? What object should the ego vehicle notice third? What is the state of the object perceived by the ego vehicle as third and what action should the ego vehicle take?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [3],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_2'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'Would <c1,CAM_BACK,384.2,477.5> be in the moving direction of the ego vehicle?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_3'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'Would <c2,CAM_BACK,1517.8,571.7> be in the moving direction of the ego vehicle?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_4'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'Would <c3,CAM_FRONT_RIGHT,391.7,535.0> be in the moving direction of the ego vehicle?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_5'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'Will <c1,CAM_BACK,384.2,477.5> be in the moving direction of <c2,CAM_BACK,1517.8,571.7>?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_6'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'Will <c1,CAM_BACK,384.2,477.5> change its motion state based on <c2,CAM_BACK,1517.8,571.7>?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_7'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'Will <c2,CAM_BACK,1517.8,571.7> be in the moving direction of <c1,CAM_BACK,384.2,477.5>?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_8'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'}),\n",
       "  ({'Q': 'Will <c2,CAM_BACK,1517.8,571.7> change its motion state based on <c1,CAM_BACK,384.2,477.5>?',\n",
       "    'A': '',\n",
       "    'C': None,\n",
       "    'con_up': None,\n",
       "    'con_down': None,\n",
       "    'cluster': None,\n",
       "    'layer': None,\n",
       "    'tag': [0],\n",
       "    'id': 'b789de07180846cc972118ee6d1fb027_b0e6fd5561454b2789c853e5350557a8_9'},\n",
       "   {'CAM_FRONT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT/n008-2018-09-18-14-35-12-0400__CAM_FRONT__1537295990612404.jpg',\n",
       "    'CAM_FRONT_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_LEFT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_LEFT__1537295990604799.jpg',\n",
       "    'CAM_FRONT_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_FRONT_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_FRONT_RIGHT__1537295990620482.jpg',\n",
       "    'CAM_BACK': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK/n008-2018-09-18-14-35-12-0400__CAM_BACK__1537295990637558.jpg',\n",
       "    'CAM_BACK_LEFT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_LEFT/n008-2018-09-18-14-35-12-0400__CAM_BACK_LEFT__1537295990647405.jpg',\n",
       "    'CAM_BACK_RIGHT': '/home/cvrr/Desktop/EfficientDriveLM/data/drivelm_nus_imgs_train/nuscenes/samples/CAM_BACK_RIGHT/n008-2018-09-18-14-35-12-0400__CAM_BACK_RIGHT__1537295990628113.jpg'})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T22:24:54.231272Z",
     "start_time": "2024-05-14T22:24:44.117860Z"
    }
   },
   "source": [
    "multi_frame_dset = [(multi_frame_train, 'multi_frame_train_DriveLM-challenge.json'),\n",
    " (multi_frame_val, 'multi_frame_val_DriveLM-challenge.json')]\n",
    "# multi_frame_dset = [(multi_frame_test, 'multi_frame_test_challenge_only_q.json')]\n",
    "\n",
    "for dset, fname in multi_frame_dset:\n",
    "\n",
    "  with open(os.path.join('../EM-VLM4AD', 'data', 'multi_frame', fname), 'w') as f:\n",
    "    json.dump(dset, f, indent=6)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaUDU3HHIYh1"
   },
   "source": [
    "# Pair Each Question with its Corresponding Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2967,
     "status": "ok",
     "timestamp": 1707617734407,
     "user": {
      "displayName": "Akshay Gopalkrishnan",
      "userId": "04646250786258962279"
     },
     "user_tz": 480
    },
    "id": "nqTEpeYhTbnc",
    "outputId": "fbbfac19-064f-4c41-9529-761a32294e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377983, 198595, 0.5254072273091647)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAM_FRONT, CAM_FRONT_LEFT, CAM_FRONT_RIGHT = 'CAM_FRONT', 'CAM_FRONT_LEFT', 'CAM_FRONT_RIGHT'\n",
    "CAM_BACK, CAM_BACK_LEFT, CAM_BACK_RIGHT = 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT'\n",
    "front, front_left, front_right = 'front', 'front left', 'front right'\n",
    "back, back_left, back_right = 'back', 'back left', 'back right'\n",
    "\n",
    "cam_list = [(CAM_FRONT_LEFT, front_left), (CAM_FRONT_RIGHT, front_right), (CAM_FRONT, front),\n",
    "            (CAM_BACK_LEFT, back_left), (CAM_BACK_RIGHT, back_right), (CAM_BACK, back)]\n",
    "\n",
    "single_frame_qi_pairs = []\n",
    "multi_frame_qi_pairs = []\n",
    "num_questions  = 0\n",
    "\n",
    "\n",
    "for scene_token in list(data.keys()):\n",
    "\n",
    "  scene_frames = data[scene_token]['key_frames']\n",
    "\n",
    "  for frame_token in scene_frames:\n",
    "\n",
    "    frame = scene_frames[frame_token]\n",
    "\n",
    "    for q in frame['QA']:\n",
    "\n",
    "      qa_list = frame['QA'][q]\n",
    "      image_paths = scene_frames[frame_token]['image_paths']\n",
    "\n",
    "\n",
    "      for question in qa_list:\n",
    "\n",
    "        num_cams = set()\n",
    "\n",
    "        for cam, direc in cam_list:\n",
    "          if cam in question['Q'] or direc in question['Q'].lower():\n",
    "            num_cams.add((cam, direc))\n",
    "          if cam in question['A'] or direc in question['A'].lower():\n",
    "            num_cams.add((cam, direc))\n",
    "\n",
    "        num_cams = list(num_cams)\n",
    "\n",
    "        if len(num_cams) == 1:\n",
    "          # print(question['Q'], question['A'])\n",
    "          single_frame_qi_pairs.append((question, image_paths[num_cams[0][0]]))\n",
    "\n",
    "        # if CAM_FRONT in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_FRONT]))\n",
    "        # elif CAM_FRONT_RIGHT in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_FRONT_RIGHT]))\n",
    "        # elif CAM_FRONT_LEFT in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_FRONT_LEFT]))\n",
    "        # elif CAM_BACK in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_BACK]))\n",
    "        # elif CAM_BACK_RIGHT in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_BACK_RIGHT]))\n",
    "        # elif CAM_BACK_LEFT in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_BACK_LEFT]))\n",
    "        # elif front_right in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_FRONT_RIGHT]))\n",
    "        # elif front_left in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_FRONT_LEFT]))\n",
    "        # elif front in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_FRONT]))\n",
    "        # elif back_right in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_BACK]))\n",
    "        # elif back_left in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_BACK_RIGHT]))\n",
    "        # elif back in question['Q']:\n",
    "        #   single_frame_qi_pairs.append((question, image_paths[CAM_BACK_LEFT]))\n",
    "\n",
    "        multi_frame_qi_pairs.append((question, image_paths))\n",
    "\n",
    "        num_questions += 1\n",
    "\n",
    "num_questions, len(single_frame_qi_pairs), len(single_frame_qi_pairs)/num_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5918,
     "status": "ok",
     "timestamp": 1707617824290,
     "user": {
      "displayName": "Akshay Gopalkrishnan",
      "userId": "04646250786258962279"
     },
     "user_tz": 480
    },
    "id": "VSyIb61C3TDJ",
    "outputId": "1ed40339-4e96-4aed-bbd0-724762102657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198595, 377983)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the paths for the image pairs\n",
    "for i in range(len(single_frame_qi_pairs)):\n",
    "\n",
    "  single_frame_qi_pairs[i] = (single_frame_qi_pairs[i][0], os.path.join('drive', 'MyDrive', 'DriveLM', 'data') + single_frame_qi_pairs[i][1][2:])\n",
    "\n",
    "for i in range(len(multi_frame_qi_pairs)):\n",
    "\n",
    "  new_cam_set = {cam: os.path.join('drive', 'MyDrive', 'DriveLM', 'data') + multi_frame_qi_pairs[i][1][cam][2:] for cam in multi_frame_qi_pairs[i][1]}\n",
    "  multi_frame_qi_pairs[i] = (multi_frame_qi_pairs[i][0], dict(new_cam_set))\n",
    "\n",
    "\n",
    "len(single_frame_qi_pairs), len(multi_frame_qi_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3eZx6fU2um8"
   },
   "source": [
    "# Save QA-Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrCIJr5j3AzT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(single_frame_qi_pairs), random.shuffle(multi_frame_qi_pairs)\n",
    "\n",
    "single_frame_train = single_frame_qi_pairs[:int(len(single_frame_qi_pairs)*0.9)]\n",
    "single_frame_val = single_frame_qi_pairs[int(len(single_frame_qi_pairs)*0.9):int(len(single_frame_qi_pairs)*0.95)]\n",
    "single_frame_test = single_frame_qi_pairs[int(len(single_frame_qi_pairs)*0.95):]\n",
    "\n",
    "\n",
    "multi_frame_train = multi_frame_qi_pairs[:int(len(multi_frame_qi_pairs)*0.9)]\n",
    "multi_frame_val = multi_frame_qi_pairs[int(len(multi_frame_qi_pairs)*0.9):int(len(multi_frame_qi_pairs)*0.95)]\n",
    "multi_frame_test = multi_frame_qi_pairs[int(len(multi_frame_qi_pairs)*0.95):]\n",
    "\n",
    "# os.mkdir(os.path.join('drive', 'MyDrive', 'DriveLM', 'single_frame'))\n",
    "# os.mkdir(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame'))\n",
    "\n",
    "single_frame_dset = [(single_frame_train, 'single_frame_train.json'),\n",
    " (single_frame_val, 'single_frame_val.json'), (single_frame_test, 'single_frame_test.json')]\n",
    "multi_frame_dset = [(multi_frame_train, 'multi_frame_train.json'),\n",
    " (multi_frame_val, 'multi_frame_val.json'), (multi_frame_test, 'multi_frame_test.json')]\n",
    "\n",
    "for dset, fname in single_frame_dset:\n",
    "\n",
    "  with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'data', 'single_frame', fname), 'w') as f:\n",
    "    json.dump(dset, f, indent=6)\n",
    "\n",
    "for dset, fname in multi_frame_dset:\n",
    "\n",
    "  with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'data', 'multi_frame', fname), 'w') as f:\n",
    "    json.dump(dset, f, indent=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHL2u6-n9Hxt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNf6XoS3CDPqq/EvAtde2Tb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
